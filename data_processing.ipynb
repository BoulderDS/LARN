{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse NOW corpus text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_strs defines the scope of our dataset.\n",
    "\n",
    "Processing US news from January 2016 to June 2018 would take about 1 hour.\n",
    "\n",
    "Processing all news from January 2016 to June 2018 would take half a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '/data/NOW/text/'\n",
    "write_path = '/data/ent2ent/han/incremental/'\n",
    "filter_strs = ['18*', 'text_18*', '17*', 'text_17*', '16*', 'text_16*'] # change this to '*' for all files\n",
    "year_base = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we manually define some entities and their aliases.\n",
    "\n",
    "We are to consider every pair within the combination of these entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_aliases_dict = {'U.S.': {'U.S.', 'US', 'USA', 'Trump', 'Obama'},\n",
    "                       'China': {'China', 'Chinese', 'Xi'},\n",
    "                       'Syria': {'Syria', 'Syrian', 'Assad'},\n",
    "                       'France': {'France', 'French', 'Macron', 'Hollande'},\n",
    "                       'Germany': {'Germany', 'German', 'Merkel'},\n",
    "                       'Canada': {'Canada', 'Canadian', 'Trudeau'},\n",
    "                       'Russia': {'Russia', 'Russian', 'Putin'},\n",
    "                       'India': {'India', 'Indian', 'Modi'},\n",
    "                       'U.K.': {'U.K.', 'UK', 'British', 'Britain', 'Cameron'},\n",
    "                       'Japan': {'Japan', 'Japanese', 'Abe'},\n",
    "                       'Iran': {'Iran', 'Iranian', 'Khamenei', 'Rouhani'},\n",
    "                       'Israel': {'Israel', 'Israeli', 'Netanyahu'}}\n",
    "entities_list = list(entity_aliases_dict.keys())\n",
    "interest_pair_list = []\n",
    "for i in range(len(entities_list) - 1):\n",
    "    for j in range(i + 1, len(entities_list)):\n",
    "        interest_pair_list.append((entities_list[i], entities_list[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some util functions for Subject-Verb-Object (SVO) extraction.\n",
    "\n",
    "Code adapted from: https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "REL_PRONS = [\"that\", \"who\", \"which\", \"whom\", \"whose\", \"where\", \"when\", \"what\", \"why\"]\n",
    "\n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\" and tok.lower_ not in REL_PRONS]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS and tok.lower_ not in REL_PRONS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findSVOs() does the following things:\n",
    "1. find the subject, predicate, and object of an input sentence\n",
    "2. if the predicate contains negation, we replace it with its antonym (if no antonym found, we report no SVO)\n",
    "3. return the subject, predicate, and object, all in lemma forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    if verbNegated or objNegated: # if negative word, get the antonym\n",
    "                        neg_v = None\n",
    "                        found = False\n",
    "                        for syn in wordnet.synsets(v.lower_):\n",
    "                            if found:\n",
    "                                break\n",
    "                            if syn.pos() != 'v':\n",
    "                                continue\n",
    "                            for l in syn.lemmas():\n",
    "                                if l.antonyms():\n",
    "                                    neg_v = l.antonyms()[0].name()\n",
    "                                    found = True\n",
    "                        if neg_v != None:\n",
    "                            s_lemma = sub.lemma_ if sub.lemma_ != '-PRON-' else sub.lower_ # spacy's pronoun lemma hack\n",
    "                            o_lemma = obj.lemma_ if obj.lemma_ != '-PRON-' else obj.lower_\n",
    "                            svos.append((s_lemma, neg_v, o_lemma))\n",
    "                    else:\n",
    "                        s_lemma = sub.lemma_ if sub.lemma_ != '-PRON-' else sub.lower_\n",
    "                        v_lemma = v.lemma_\n",
    "                        o_lemma = obj.lemma_ if obj.lemma_ != '-PRON-' else obj.lower_\n",
    "                        svos.append((s_lemma, v_lemma, o_lemma))\n",
    "    return svos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findGenerals() does the following things:\n",
    "1. find all nouns (including proper nouns) in the input sentence\n",
    "2. find all verbs, except auxiliary verbs, in the input sentence\n",
    "3. find all adjectives in the input sentence\n",
    "4. find all adverbs in the input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGenerals(tokens):\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjs = []\n",
    "    advs = []\n",
    "    all_words = []\n",
    "    for tok in tokens:\n",
    "        t_lemma = tok.lemma_ if tok.lemma_ != '-PRON-' else tok.lower_\n",
    "        all_words.append(t_lemma)\n",
    "        if tok.pos_ == \"NOUN\" or tok.pos_ == \"PROPN\":\n",
    "            nouns.append(t_lemma)\n",
    "        elif tok.pos_ == \"VERB\" and tok.dep_ != \"aux\":\n",
    "            verbs.append(t_lemma)\n",
    "        elif tok.pos_ == \"ADJ\":\n",
    "            adjs.append(t_lemma)\n",
    "        elif tok.pos_ == \"ADV\":\n",
    "            advs.append(t_lemma)\n",
    "    return nouns, verbs, adjs, advs, all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_related_entities() takes in the whole news article and determine a list of entity pairs that appear in the article.\n",
    "\n",
    "Note that here we need both entities in the entity pair to appear in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_entities(line, interest_pair_list):\n",
    "    related_interest_pairs = []\n",
    "    for interest_pair in interest_pair_list:\n",
    "        e1_appears = False\n",
    "        e2_appears = False\n",
    "        for e1 in entity_aliases_dict[interest_pair[0]]:\n",
    "            if e1 in line:\n",
    "                e1_appears = True\n",
    "        for e2 in entity_aliases_dict[interest_pair[1]]:\n",
    "            if e2 in line:\n",
    "                e2_appears = True\n",
    "        if e1_appears and e2_appears:\n",
    "            related_interest_pairs.append(interest_pair)\n",
    "    return related_interest_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process_news_article() is the main function for processing every news article.\n",
    "\n",
    "Before introducing how this function works, we should first look at a typical news article in the NOW corpus:\n",
    "\n",
    "\"@@12345 <h\\> HEADER OF THE NEWS <p\\> First paragraph's text <p\\> Second paragraph's text <p\\> ...\", where \"12345\" is the id of the news article. The larger id, the later crawled.\n",
    "\n",
    "process_news_article() does the following things:\n",
    "1. split a news article to news id, header, and paragraphs\n",
    "2. for the header and each paragraph, we first check if it contains any entity pair (need both entities within the pair to appear)\n",
    "3. if true, we segment the paragraph into sentences and check every sentence if it contains any entity pair (also need both entities to appear)\n",
    "4. if true, we find the SVOs, nouns, verbs, adjectives, and adverbs in the sentence\n",
    "5. finally we aggregate all the SVOs, nouns, verbs, adjectives, and adverbs found in the article and return them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_article(line, interest_pair_list):\n",
    "    split_list = re.split(r'<.>', line) # split to paragraphs using <h> and <p>\n",
    "    \n",
    "    news_svo_info = defaultdict(list)\n",
    "    news_nn_info = defaultdict(list)\n",
    "    news_vb_info = defaultdict(list)\n",
    "    news_jj_info = defaultdict(list)\n",
    "    news_rb_info = defaultdict(list)\n",
    "    news_all_info = defaultdict(list)\n",
    "    news_samples = defaultdict(list)\n",
    "    \n",
    "    if len(split_list) <= 1:\n",
    "        return 0, news_svo_info, news_nn_info, news_vb_info, news_jj_info, news_rb_info, news_all_info, news_samples\n",
    "    \n",
    "    try:\n",
    "        news_index = int(split_list[0][2:])\n",
    "    except:\n",
    "        return 0, news_svo_info, news_nn_info, news_vb_info, news_jj_info, news_rb_info, news_all_info, news_samples\n",
    "    \n",
    "    for split in split_list[1:]:\n",
    "        related_interest_pairs = []\n",
    "        for interest_pair in interest_pair_list:\n",
    "            e1_appears = False\n",
    "            e2_appears = False\n",
    "            for e1 in entity_aliases_dict[interest_pair[0]]:\n",
    "                if e1 in split:\n",
    "                    e1_appears = True\n",
    "            for e2 in entity_aliases_dict[interest_pair[1]]:\n",
    "                if e2 in split:\n",
    "                    e2_appears = True\n",
    "            if e1_appears and e2_appears: # if paragraph contains both entities\n",
    "                related_interest_pairs.append(interest_pair)\n",
    "            \n",
    "        if len(related_interest_pairs) == 0: # paragraph not containing any entity pair\n",
    "            continue\n",
    "            \n",
    "        doc = nlp(split)\n",
    "        for sent in doc.sents:\n",
    "            if '@ @' in sent.text: # broken sentence\n",
    "                continue\n",
    "                \n",
    "            is_target_sent = False\n",
    "            sent_related_interest_pairs = []\n",
    "            for interest_pair in related_interest_pairs:\n",
    "                e1_appears = False\n",
    "                e2_appears = False\n",
    "                for e1 in entity_aliases_dict[interest_pair[0]]:\n",
    "                    if e1 in sent.text:\n",
    "                        e1_appears = True\n",
    "                for e2 in entity_aliases_dict[interest_pair[1]]:\n",
    "                    if e2 in sent.text:\n",
    "                        e2_appears = True\n",
    "                if e1_appears and e2_appears: # if sentence has both entities\n",
    "                    is_target_sent = True\n",
    "                    sent_related_interest_pairs.append(interest_pair)\n",
    "                    \n",
    "            if is_target_sent: # sentence of interest\n",
    "                for rip in sent_related_interest_pairs:\n",
    "                    news_svo_info[rip].extend(findSVOs(sent))\n",
    "                    nouns, verbs, adjs, advs, all_words = findGenerals(sent)\n",
    "                    news_nn_info[rip].extend(nouns)\n",
    "                    news_vb_info[rip].extend(verbs)\n",
    "                    news_jj_info[rip].extend(adjs)\n",
    "                    news_rb_info[rip].extend(advs)\n",
    "                    news_all_info[rip].extend(all_words)\n",
    "                    news_samples[rip].append(sent.text)\n",
    "                    \n",
    "    return news_index, news_svo_info, news_nn_info, news_vb_info, news_jj_info, news_rb_info,\\\n",
    "        news_all_info, news_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run process_news_article() on all articles in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_file(filename):\n",
    "    f = open(filename, 'r')\n",
    "\n",
    "    if 'text_' in filename:\n",
    "        time = filename[len(load_path) + 5: len(load_path) + 10]\n",
    "        year = time[:2]\n",
    "        month = time[-2:]\n",
    "        time = (int(year) - year_base) * 12 + int(month)\n",
    "        short_filename = filename[len(load_path) + 5: -4]\n",
    "    else:\n",
    "        time = filename[len(load_path): len(load_path) + 5]\n",
    "        year = time[:2]\n",
    "        month = time[-2:]\n",
    "        time = (int(year) - year_base) * 12 + int(month)\n",
    "        short_filename = filename[len(load_path): -4]\n",
    "\n",
    "    news_list = f.readlines()\n",
    "    entity_svo_dict = defaultdict(dict)\n",
    "    entity_nn_dict = defaultdict(dict)\n",
    "    entity_vb_dict = defaultdict(dict)\n",
    "    entity_jj_dict = defaultdict(dict)\n",
    "    entity_rb_dict = defaultdict(dict)\n",
    "    entity_all_dict = defaultdict(dict)\n",
    "    entity_sample_dict = defaultdict(dict)\n",
    "\n",
    "    for news in news_list:\n",
    "        related_pairs = find_related_entities(news, interest_pair_list)\n",
    "        if len(related_pairs) > 0:\n",
    "            news_index, news_svo_info, news_nn_info, news_vb_info, news_jj_info, news_rb_info,\\\n",
    "                news_all_info, news_samples = process_news_article(news, related_pairs)\n",
    "            for k,v in news_svo_info.items():\n",
    "                entity_svo_dict[k][(time, news_index)] = v\n",
    "            for k,v in news_nn_info.items():\n",
    "                entity_nn_dict[k][(time, news_index)] = v\n",
    "            for k,v in news_vb_info.items():\n",
    "                entity_vb_dict[k][(time, news_index)] = v\n",
    "            for k,v in news_jj_info.items():\n",
    "                entity_jj_dict[k][(time, news_index)] = v\n",
    "            for k,v in news_rb_info.items():\n",
    "                entity_rb_dict[k][(time, news_index)] = v\n",
    "            for k,v in news_all_info.items():\n",
    "                entity_all_dict[k][(time, news_index)] = v\n",
    "            for k,v in news_samples.items():\n",
    "                entity_sample_dict[k][(time, news_index)] = '\\n'.join(v)\n",
    "    \n",
    "    f.close()\n",
    "    pickle.dump(entity_svo_dict, open(write_path + 'large_svo_' + short_filename + '.pkl', 'wb'))\n",
    "    pickle.dump(entity_nn_dict, open(write_path + 'large_nn_' + short_filename + '.pkl', 'wb'))\n",
    "    pickle.dump(entity_vb_dict, open(write_path + 'large_vb_' + short_filename + '.pkl', 'wb'))\n",
    "    pickle.dump(entity_jj_dict, open(write_path + 'large_jj_' + short_filename + '.pkl', 'wb'))\n",
    "    pickle.dump(entity_rb_dict, open(write_path + 'large_rb_' + short_filename + '.pkl', 'wb'))\n",
    "    pickle.dump(entity_all_dict, open(write_path + 'large_all_' + short_filename + '.pkl', 'wb'))\n",
    "    pickle.dump(entity_sample_dict, open(write_path + 'large_sample_' + short_filename + '.pkl', 'wb'))\n",
    "    print(short_filename, 'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture process_time\n",
    "%%time\n",
    "\n",
    "filename_list = []\n",
    "for filter_str in filter_strs:\n",
    "    for filename in glob.glob(load_path + filter_str + '.txt'):\n",
    "        filename_list.append(filename)\n",
    "\n",
    "with Pool(40) as p: # fork 40 processes\n",
    "    p.map(process_news_file, filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the entity pairs and the lexical data of all articles processed in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '/data/ent2ent/han/incremental/'\n",
    "write_path = '/data/ent2ent/han/incremental/'\n",
    "meta_field_list = ['Internation']\n",
    "\n",
    "entity_aliases_dict = {'U.S.': {'U.S.', 'US', 'USA', 'Trump', 'Obama'},\n",
    "                       'China': {'China', 'Chinese', 'Xi'},\n",
    "                       'Syria': {'Syria', 'Syrian', 'Assad'},\n",
    "                       'France': {'France', 'French', 'Macron', 'Hollande'},\n",
    "                       'Germany': {'Germany', 'German', 'Merkel'},\n",
    "                       'Canada': {'Canada', 'Canadian', 'Trudeau'},\n",
    "                       'Russia': {'Russia', 'Russian', 'Putin'},\n",
    "                       'India': {'India', 'Indian', 'Modi'},\n",
    "                       'U.K.': {'U.K.', 'UK', 'British', 'Britain', 'Cameron'},\n",
    "                       'Japan': {'Japan', 'Japanese', 'Abe'},\n",
    "                       'Iran': {'Iran', 'Iranian', 'Khamenei', 'Rouhani'},\n",
    "                       'Israel': {'Israel', 'Israeli', 'Netanyahu'}}\n",
    "entities_list = list(entity_aliases_dict.keys())\n",
    "entity_pair_list = []\n",
    "for i in range(len(entities_list) - 1):\n",
    "    for j in range(i + 1, len(entities_list)):\n",
    "        entity_pair_list.append((entities_list[i], entities_list[j]))\n",
    "\n",
    "sm_nlp = spacy.load('en_core_web_sm') # due to spacy's bug, only small version handles stop words\n",
    "lg_nlp = spacy.load('en_core_web_lg') # large version handles word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_svo_dict = defaultdict(dict)\n",
    "entity_nn_dict = defaultdict(dict)\n",
    "entity_vb_dict = defaultdict(dict)\n",
    "entity_jj_dict = defaultdict(dict)\n",
    "entity_rb_dict = defaultdict(dict)\n",
    "entity_all_dict = defaultdict(dict)\n",
    "entity_sample_dict = defaultdict(dict)\n",
    "\n",
    "for filename in glob.glob(load_path + 'large_svo*.pkl'):\n",
    "    d = pickle.load(open(filename, 'rb'))\n",
    "    for ep, ed in d.items():\n",
    "        entity_svo_dict[ep].update(ed)\n",
    "for filename in glob.glob(load_path + 'large_nn*.pkl'):\n",
    "    d = pickle.load(open(filename, 'rb'))\n",
    "    for ep, ed in d.items():\n",
    "        entity_nn_dict[ep].update(ed)\n",
    "for filename in glob.glob(load_path + 'large_vb*.pkl'):\n",
    "    d = pickle.load(open(filename, 'rb'))\n",
    "    for ep, ed in d.items():\n",
    "        entity_vb_dict[ep].update(ed)\n",
    "for filename in glob.glob(load_path + 'large_jj*.pkl'):\n",
    "    d = pickle.load(open(filename, 'rb'))\n",
    "    for ep, ed in d.items():\n",
    "        entity_jj_dict[ep].update(ed)\n",
    "for filename in glob.glob(load_path + 'large_rb*.pkl'):\n",
    "    d = pickle.load(open(filename, 'rb'))\n",
    "    for ep, ed in d.items():\n",
    "        entity_rb_dict[ep].update(ed)\n",
    "for filename in glob.glob(load_path + 'large_all*.pkl'):\n",
    "    d = pickle.load(open(filename, 'rb'))\n",
    "    for ep, ed in d.items():\n",
    "        entity_all_dict[ep].update(ed)\n",
    "for filename in glob.glob(load_path + 'large_sample*.pkl'):\n",
    "    d = pickle.load(open(filename, 'rb'))\n",
    "    for ep, ed in d.items():\n",
    "        entity_sample_dict[ep].update(ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below blocks generate the input for our entity relationship model.\n",
    "\n",
    "The input mainly contains the following things:\n",
    "1. \"span_data\" that contains trajectories of entity pair's lexical data\n",
    "2. \"target_verbs_ix_set\" that limits the word choice of our model's relationship descriptors\n",
    "3. \"We\" that saves trained GloVe word embeddings for all words appeared in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_words = set()\n",
    "\n",
    "# preprocessing for RMN\n",
    "upper_bound = 500\n",
    "lower_bound = 5000\n",
    "\n",
    "all_words_counter = Counter()\n",
    "for ep in entity_all_dict.keys():\n",
    "    for mp in entity_all_dict[ep].keys():\n",
    "        for w in entity_all_dict[ep][mp]:\n",
    "            all_words_counter[w] += 1\n",
    "print(len(all_words_counter))\n",
    "\n",
    "cnts = all_words_counter.most_common()\n",
    "for w, _ in cnts[:upper_bound]:\n",
    "    margin_words.add(w)\n",
    "for w, _ in cnts[-lower_bound:]:\n",
    "    margin_words.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_span_data = []\n",
    "\n",
    "for ep in entity_pair_list:\n",
    "    news_metadata_pair_set = list(entity_svo_dict[ep].keys())\n",
    "    metadata_ordered_list = sorted(news_metadata_pair_set, key=lambda x: x[1]) # news index reflects time order\n",
    "    raw_spans = []\n",
    "    raw_masks = []\n",
    "    raw_months = []\n",
    "    raw_samples = []\n",
    "    for i, mp in enumerate(metadata_ordered_list):\n",
    "        raw_spans_buf = []\n",
    "        raw_masks_buf = []\n",
    "        raw_samples_buf = []\n",
    "        \n",
    "        for svo in entity_svo_dict[ep][mp]:\n",
    "            if not sm_nlp.vocab[svo[0]].is_stop:\n",
    "                raw_spans_buf.append(svo[0])\n",
    "                raw_masks_buf.append(1) # s -> 1\n",
    "            if not sm_nlp.vocab[svo[1]].is_stop:\n",
    "                raw_spans_buf.append(svo[1])\n",
    "                raw_masks_buf.append(2) # v -> 2\n",
    "            if not sm_nlp.vocab[svo[2]].is_stop:\n",
    "                raw_spans_buf.append(svo[2])\n",
    "                raw_masks_buf.append(3) # o -> 3\n",
    "                \n",
    "        for nn in entity_nn_dict[ep][mp]:\n",
    "            if not sm_nlp.vocab[nn].is_stop:\n",
    "                raw_spans_buf.append(nn)\n",
    "                raw_masks_buf.append(4) # nn -> 4\n",
    "        for vb in entity_vb_dict[ep][mp]:\n",
    "            if not sm_nlp.vocab[vb].is_stop:\n",
    "                raw_spans_buf.append(vb)\n",
    "                raw_masks_buf.append(5) # vb -> 5\n",
    "        for jj in entity_jj_dict[ep][mp]:\n",
    "            if not sm_nlp.vocab[jj].is_stop:\n",
    "                raw_spans_buf.append(jj)\n",
    "                raw_masks_buf.append(6) # jj -> 6\n",
    "        for rb in entity_rb_dict[ep][mp]:\n",
    "            if not sm_nlp.vocab[rb].is_stop:\n",
    "                raw_spans_buf.append(rb)\n",
    "                raw_masks_buf.append(7) # rb -> 7\n",
    "                \n",
    "        for w in entity_all_dict[ep][mp]:\n",
    "            if w not in margin_words:\n",
    "                raw_spans_buf.append(w)\n",
    "                raw_masks_buf.append(8) # processed general words -> 8\n",
    "                \n",
    "        raw_samples_buf.append(entity_sample_dict[ep][mp])\n",
    "        \n",
    "        if (2 in raw_masks_buf): # if no predicate in the group, discard\n",
    "            raw_spans.append(raw_spans_buf)\n",
    "            raw_masks.append(raw_masks_buf)\n",
    "            raw_months.append(mp[0])\n",
    "            raw_samples.append(raw_samples_buf)\n",
    "        \n",
    "    raw_span_data.append(([meta_field_list[0]], [ep[0], ep[1]], raw_spans, raw_masks, raw_months, raw_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_field_set = set()\n",
    "entity_set = set()\n",
    "word_set = set()\n",
    "max_len = 0\n",
    "\n",
    "for rsd in raw_span_data:\n",
    "    for mf in rsd[0]:\n",
    "        meta_field_set.add(mf)\n",
    "    for ent in rsd[1]:\n",
    "        entity_set.add(ent)\n",
    "    for rs in rsd[2]:\n",
    "        if len(rs) > max_len:\n",
    "            max_len = len(rs)\n",
    "        for w in rs:\n",
    "            word_set.add(w)\n",
    "\n",
    "mf2ix = {word: i for i, word in enumerate(meta_field_set)}\n",
    "e2ix = {word: i for i, word in enumerate(entity_set)}\n",
    "w2ix = {word: i for i, word in enumerate(word_set)}\n",
    "ix2mf = {i: word for word, i in mf2ix.items()}\n",
    "ix2e = {i: word for word, i in e2ix.items()}\n",
    "ix2w = {i: word for word, i in w2ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_data = []\n",
    "target_verb_ix_set = set()\n",
    "\n",
    "predicate_ix_counter = Counter()\n",
    "for rsd in raw_span_data:\n",
    "    mf_list = [mf2ix[mf] for mf in rsd[0]]\n",
    "    ent_list = [e2ix[ent] for ent in rsd[1]]\n",
    "    spans = np.zeros([len(rsd[2]), max_len], dtype=int)\n",
    "    masks = np.zeros([len(rsd[2]), max_len], dtype=float)\n",
    "    months = np.array(rsd[4], dtype=int)\n",
    "    samples = rsd[5]\n",
    "    for i, rs in enumerate(rsd[2]):\n",
    "        for j, w in enumerate(rs):\n",
    "            spans[i][j] = w2ix[w]\n",
    "            masks[i][j] = rsd[3][i][j]\n",
    "            if masks[i][j] == 2: # all predicates\n",
    "                predicate_ix_counter[spans[i][j]] += 1\n",
    "    span_data.append((mf_list, ent_list, spans, masks, months, samples))\n",
    "\n",
    "for wix, _ in predicate_ix_counter.most_common()[:500]:\n",
    "    target_verb_ix_set.add(wix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predicate_ix_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We = np.zeros([len(ix2w), 300], dtype=float)\n",
    "\n",
    "for i, word in ix2w.items():\n",
    "    We[i] = lg_nlp.vocab[word].vector\n",
    "    \n",
    "norms = np.linalg.norm(We, axis=1)\n",
    "\n",
    "for i, emb in enumerate(We):\n",
    "    if norms[i] > 0:\n",
    "        for j in range(len(emb)):\n",
    "            We[i][j] /= norms[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = (ix2mf, ix2e, w2ix, ix2w, span_data, max_len, target_verb_ix_set, We)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_input, open(write_path + 'large_model_input_gs1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of entity pair occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pickle.load(open(write_path + 'large_model_input_gs1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix2mf, ix2e, w2ix, ix2w, span_data, max_len, target_verb_ix_set, We = model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article\n",
    "ent_dist = sorted([(ix2e[e_list[0]], ix2e[e_list[1]], len(span)) for _, e_list, span, _,_,_ in span_data],\n",
    "                  key=lambda x: -x[2])\n",
    "num_nation_pair_related_articles = 0\n",
    "for ed in ent_dist:\n",
    "    num_nation_pair_related_articles += ed[2]\n",
    "print(num_nation_pair_related_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nation_pair_related_sentences = 0\n",
    "for _, e_list, span, _, _, article_samples in span_data:\n",
    "    for article_sample in article_samples:\n",
    "        sentence_split = article_sample[0].split('\\n') # index 0 since we have one extra list out article_sample\n",
    "        num_nation_pair_related_sentences += len(sentence_split)\n",
    "print(num_nation_pair_related_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dist = sorted([(ix2e[e_list[0]], ix2e[e_list[1]], len(span)) for _, e_list, span, _,_,_ in span_data],\n",
    "                  key=lambda x: -x[2])\n",
    "for ed in ent_dist:\n",
    "    print(ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of predicates per span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_list = []\n",
    "for _, _, _, masks, _, _ in span_data:\n",
    "    for mask in masks:\n",
    "        cnt = 0\n",
    "        for m in mask:\n",
    "            if m == 2:\n",
    "                cnt += 1\n",
    "        avg_list.append(cnt)\n",
    "print(1.0 * sum(avg_list) / len(avg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of nouns per span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_list = []\n",
    "for _, _, _, masks, _, _ in span_data:\n",
    "    for mask in masks:\n",
    "        cnt = 0\n",
    "        for m in mask:\n",
    "            if m == 4: # nouns\n",
    "                cnt += 1\n",
    "        avg_list.append(cnt)\n",
    "print(1.0 * sum(avg_list) / len(avg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
