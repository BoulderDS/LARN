{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "from utils import *\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'larn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmap, cmap, wmap, revmap, span_data, span_size, target_pred_ix_set, We = load_data(data_load_path)\n",
    "\n",
    "num_chars, num_books, num_traj = len(cmap), len(bmap), len(span_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_log = 'descriptors_model_' + str(model_id) + '.log'\n",
    "\n",
    "trajectory_log = 'trajectories_model_' + str(model_id) + '.log'\n",
    "\n",
    "desc_sample_file_name = 'desc_selected_sample_dict_model_' + str(model_id) + '.pkl'\n",
    "\n",
    "attn_sample_file_name = 'attn_selected_sample_dict_model_' + str(model_id) + '.pkl'\n",
    "\n",
    "training_progress_log = 'training_progress_log_' + str(model_id) + '.txt'\n",
    "\n",
    "model_object_file_name = 'trained_model_' + str(model_id) + '.pt'\n",
    "\n",
    "A_dict_file_name = 'A_dict_' + str(model_id) + '.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LARN(nn.Module):\n",
    "\n",
    "    def __init__(self, d_word, d_noun_hidden, d_ent, d_meta, d_mix, d_desc, n_ent, n_meta, We):\n",
    "        super(LARN, self).__init__()\n",
    "        self.d_desc = d_desc\n",
    "        tensor_We = torch.FloatTensor(We).to(device)\n",
    "        self.l_st = TrainedWordEmbeddingLayer(tensor_We, d_word)\n",
    "        self.l_noun = NounAttentionLayer_SingleQuery(tensor_We, d_word, d_noun_hidden, d_desc)\n",
    "        self.l_ent = nn.Embedding(n_ent, d_ent)\n",
    "        self.l_mix = MixingLayer_Attention_SingleQuery_Concat(d_word, d_noun_hidden, d_ent, d_mix)\n",
    "        self.l_rels = DistributionLayer(d_mix, d_desc)\n",
    "        self.l_recon = nn.Linear(d_desc, d_word)\n",
    "        nn.init.xavier_uniform_(self.l_recon.weight)\n",
    "        \n",
    "    def init_attention(self, A):\n",
    "        self.l_noun.query.weight = nn.Parameter(A)\n",
    "\n",
    "    def forward(self, spans, noun_spans, e1, e2, months):\n",
    "        e1_tensor = torch.LongTensor(e1).to(device)\n",
    "        e2_tensor = torch.LongTensor(e2).to(device)\n",
    "        \n",
    "        outputs_l_st = self.l_st(spans)\n",
    "        outputs_l_noun = self.l_noun(noun_spans, months)\n",
    "        outputs_l_e1 = self.l_ent(e1_tensor).expand(len(spans), -1)\n",
    "        outputs_l_e2 = self.l_ent(e2_tensor).expand(len(spans), -1)\n",
    "        \n",
    "        outputs_l_mix = self.l_mix(outputs_l_st, outputs_l_noun, outputs_l_e1, outputs_l_e2)\n",
    "        \n",
    "        outputs_l_rels = self.l_rels(outputs_l_mix)\n",
    "        \n",
    "        outputs = self.l_recon(outputs_l_rels)\n",
    "        return outputs, outputs_l_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LARN(d_word, d_noun_hidden, d_char, d_book, d_mix, num_descs, num_chars, num_books, We).to(device)\n",
    "loss_function = Contrastive_Max_Margin_Loss().to(device)\n",
    "optimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()), lr=lr)\n",
    "label_generation_layer = TrainedWordEmbeddingLayer(torch.FloatTensor(We), d_word).to(device)\n",
    "\n",
    "A_dict = dict()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_start_t = time.time()\n",
    "    epoch_total_loss = torch.tensor(0.).to(device)\n",
    "    random.shuffle(span_data)\n",
    "    \n",
    "    for book, chars, big_spans, big_masks, big_months, _ in span_data:\n",
    "        char1 = [chars[0]]\n",
    "        char2 = [chars[1]]\n",
    "        \n",
    "        zip_list_to_shuffle = list(zip(big_spans, big_masks, big_months))\n",
    "        random.shuffle(zip_list_to_shuffle)\n",
    "        shuffled_big_spans, shuffled_big_masks, shuffled_big_months = zip(*zip_list_to_shuffle)\n",
    "        \n",
    "        split_indices = [i for i in range(0, len(shuffled_big_spans), batch_size)] # split to batches to fit in memory\n",
    "        spans_split = np.split(shuffled_big_spans, split_indices)\n",
    "        masks_split = np.split(shuffled_big_masks, split_indices)\n",
    "        months_split = np.split(shuffled_big_months, split_indices)\n",
    "        \n",
    "        for spans, masks, months in zip(spans_split, masks_split, months_split):\n",
    "            if len(spans) != batch_size:\n",
    "                continue\n",
    "        \n",
    "            model.zero_grad()\n",
    "\n",
    "            if (chars[0], chars[1]) in A_dict:\n",
    "                model.init_attention(A_dict[(chars[0], chars[1])].to(device))\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(model.l_noun.query.weight)\n",
    "\n",
    "            train_masked_spans = []\n",
    "            noun_spans = []\n",
    "            drop_masks = (np.random.rand(*(masks.shape)) < (1 - p_drop)).astype('float32')\n",
    "            for span_index, (span, mask, drop_mask) in enumerate(zip(spans, masks, drop_masks)):\n",
    "                train_masked_span = [span[i] for i in range(len(span))\n",
    "                                     if mask[i] == predicate_ix and drop_mask[i] == 1]\n",
    "                train_masked_spans.append(train_masked_span)\n",
    "                noun_span = [span[i] for i in range(len(span)) if mask[i] == noun_ix]\n",
    "                noun_spans.append(noun_span)\n",
    "\n",
    "            pos_masked_spans = []\n",
    "            for span_index, (span, mask) in enumerate(zip(spans, masks)):\n",
    "                pos_masked_span = [span[i] for i in range(len(span)) if mask[i] == predicate_ix]\n",
    "                pos_masked_spans.append(pos_masked_span)\n",
    "\n",
    "            neg_spans, neg_masks = generate_negative_samples(num_traj, span_size,\n",
    "                                                             num_negs, span_data)\n",
    "            neg_masked_spans = []\n",
    "            for span_index, (span, mask) in enumerate(zip(neg_spans, neg_masks)):\n",
    "                neg_masked_span = [span[i] for i in range(len(span)) if mask[i] == predicate_ix]\n",
    "                neg_masked_spans.append(neg_masked_span)\n",
    "\n",
    "            outputs, _outputs_l_rels = model(train_masked_spans, noun_spans, char1, char2, months)\n",
    "            pos_labels = label_generation_layer(pos_masked_spans)\n",
    "            neg_labels = label_generation_layer(neg_masked_spans)\n",
    "\n",
    "            R = torch.t(model.l_recon.weight)\n",
    "\n",
    "            loss = loss_function(outputs, pos_labels, neg_labels, len(spans), R, eps, d_word)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_total_loss += loss\n",
    "\n",
    "            A = model.l_noun.query.weight.detach().cpu()\n",
    "            A_dict[(chars[0], chars[1])] = A\n",
    "    \n",
    "    print(\"epoch %d finished in %.2f seconds\" % (epoch, (time.time() - epoch_start_t)))\n",
    "    print(epoch_total_loss)\n",
    "    f_tpl = open(training_progress_log, 'a')\n",
    "    f_tpl.write('epoch ' + str(epoch) + ': ' + str(time.time() - epoch_start_t) + '\\n')\n",
    "    f_tpl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_object_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(A_dict, open(A_dict_file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_object_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dict = pickle.load(open(A_dict_file_name, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_ix_set = set()\n",
    "target_word_ix_counter = Counter()\n",
    "\n",
    "for _, _, spans, masks, _ , _ in span_data:\n",
    "    for span, mask in zip(spans, masks):\n",
    "        for w, m in zip(span, mask):\n",
    "            if m == predicate_ix:\n",
    "                target_word_ix_counter[w] += 1\n",
    "                \n",
    "for wix, _ in target_word_ix_counter.most_common()[:500]: # can also do without this 500 top word limit\n",
    "    target_word_ix_set.add(wix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log = open(descriptor_log, 'w')\n",
    "\n",
    "R = torch.t(model.l_recon.weight).detach().cpu().numpy()\n",
    "for ind in range(len(R)):\n",
    "    desc = R[ind] / np.linalg.norm(R[ind])\n",
    "    top_desc_list = []\n",
    "    \n",
    "    sims = We.dot(np.transpose(desc))\n",
    "    ordered_words = np.argsort(sims)[::-1]\n",
    "\n",
    "    desc_list = []\n",
    "    num_satisfied = 0\n",
    "    for w in ordered_words:\n",
    "        if num_satisfied >= num_descs_choice:\n",
    "            break\n",
    "        if w in target_word_ix_set:\n",
    "            desc_list.append(revmap[w])\n",
    "            num_satisfied += 1\n",
    "    \n",
    "    sentiment_score = calc_desc_sentiment(desc_list)\n",
    "    top_desc_list.append('-'.join(desc_list) + ' [' + str(sentiment_score) + ']')\n",
    "\n",
    "    print('descriptor %d:' % ind)\n",
    "    print(top_desc_list)\n",
    "    print()\n",
    "\n",
    "    log.write(' '.join(top_desc_list) + '\\n')\n",
    "log.flush()\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Trend Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict = defaultdict(dict)\n",
    "desc_dist_dict = dict()\n",
    "\n",
    "tlog = open(trajectory_log, 'w')\n",
    "traj_writer = csv.writer(tlog)\n",
    "traj_writer.writerow(['Book', 'Char 1', 'Char 2', 'Span ID'] + ['Topic ' + str(i) for i in range(num_descs)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for book, chars, spans, masks, months, samples in span_data:\n",
    "        c1_name, c2_name = [cmap[c] for c in chars]\n",
    "        b_name = bmap[book[0]]\n",
    "        rel = c1_name + ' AND ' + c2_name\n",
    "        \n",
    "        if rel not in interest_pairs:\n",
    "            continue\n",
    "\n",
    "        sample_dict[b_name][c1_name+' AND '+c2_name] = dict()\n",
    "        for span_index, sample in enumerate(samples):\n",
    "            sample_dict[b_name][c1_name+' AND '+c2_name][span_index] = sample\n",
    "\n",
    "        char1 = [chars[0]]\n",
    "        char2 = [chars[1]]\n",
    "        model.zero_grad()\n",
    "        model.init_attention(A_dict[(chars[0], chars[1])].to(device))\n",
    "\n",
    "        masked_spans = []\n",
    "        noun_spans = []\n",
    "        for span_index, (span, mask) in enumerate(zip(spans, masks)):\n",
    "            masked_span = [span[i] for i in range(len(span)) if mask[i] == predicate_ix]\n",
    "            masked_spans.append(masked_span)\n",
    "            noun_span = [span[i] for i in range(len(span)) if mask[i] == noun_ix]\n",
    "            noun_spans.append(noun_span)\n",
    "\n",
    "        # rerun input through model\n",
    "        _outputs, outputs_l_rels = model(masked_spans, noun_spans, char1, char2, months)\n",
    "        \n",
    "        # save descriptor distribution data\n",
    "        desc_dist_dict[rel] = dict()\n",
    "        for span_index, olr in enumerate(outputs_l_rels.detach().cpu().numpy()):\n",
    "            traj_writer.writerow([b_name, c1_name, c2_name, months[span_index], span_index] + [o for o in olr])\n",
    "            desc_dist_dict[rel][span_index] = olr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dict = dict() # for visualization\n",
    "num_nouns_choice = 10\n",
    "num_top_descs = 3 # to get an overview for all descriptors, change this to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_selected_sample_dict = defaultdict(dict) # key: (int_desc_index, int_month_info)\n",
    "major_desc_sum_dict = defaultdict(float)\n",
    "\n",
    "rmn_traj = read_csv(trajectory_log)\n",
    "rmn_descs = read_descriptors(descriptor_log)\n",
    "\n",
    "for book in rmn_traj:\n",
    "    for rel in rmn_traj[book]:\n",
    "        if rel not in interest_pairs:\n",
    "            continue\n",
    "            \n",
    "        desc_selected_sample_dict[book][rel] = defaultdict(list)\n",
    "        plt.close()\n",
    "        \n",
    "        rtraj = rmn_traj[book][rel]['distributions']\n",
    "        mtraj = rmn_traj[book][rel]['months']\n",
    "        itraj = rmn_traj[book][rel]['span_index']\n",
    "        \n",
    "        data_d = dict()\n",
    "        desc_sum_d = defaultdict(float)\n",
    "        trivial_descs = set()\n",
    "        \n",
    "        # find non-trivial descs\n",
    "        for i in range(num_descs):\n",
    "            data_d[i] = defaultdict(list)\n",
    "        for r, m in zip(rtraj, mtraj):\n",
    "            for i, desc in enumerate(r):\n",
    "                data_d[i][m].append(desc)\n",
    "        for i in data_d.keys():\n",
    "            trivial = True\n",
    "            for m in data_d[i].keys():\n",
    "                desc_sum_d[i] += np.mean(data_d[i][m])\n",
    "\n",
    "        top_ds = [top_d[0] for top_d in sorted([(i, share) for i, share in desc_sum_d.items()],\n",
    "                                               key=lambda x: -x[1])[:num_top_descs]] # get top three descriptors\n",
    "        \n",
    "        # to get which relation to use in attended word heatmap\n",
    "        print(rel)\n",
    "        print(rmn_descs[top_ds[0]], rmn_descs[top_ds[1]], rmn_descs[top_ds[2]])\n",
    "        \n",
    "        seaborn_d = {'month_info': [], 'desc_share': [], 'desc_type': []}\n",
    "        desc_share_dict = defaultdict(list)\n",
    "        \n",
    "        for r, m, span_index in zip(rtraj, mtraj, itraj):\n",
    "            for i, desc in enumerate(r):\n",
    "                if i not in top_ds:\n",
    "                    continue\n",
    "                desc_share_dict[(i, m)].append((desc, span_index))\n",
    "                seaborn_d['month_info'].append('20' + month_to_str(m, year_base))\n",
    "                seaborn_d['desc_share'].append(desc)\n",
    "                seaborn_d['desc_type'].append(i)\n",
    "                major_desc_sum_dict[i] += desc\n",
    "                \n",
    "        for k in desc_share_dict.keys():\n",
    "            desc_share_dict[k].sort(key=lambda x: -x[0])\n",
    "            if len(desc_share_dict[k]) < sample_sel_num:\n",
    "                continue\n",
    "            for sel_i in range(sample_sel_num):\n",
    "                desc_selected_sample_dict[book][rel][k].append(sample_dict[book][rel][desc_share_dict[k][sel_i][1]])\n",
    "        \n",
    "        vis_dict[rel] = [seaborn_d]\n",
    "\n",
    "pickle.dump(desc_selected_sample_dict, open(desc_sample_file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attended Words Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_selected_sample_dict = defaultdict(dict) # key: (int_desc_index, int_month_info): (word)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for book, chars, spans, masks, months, samples in span_data:\n",
    "        c1_name, c2_name = [cmap[c] for c in chars]\n",
    "        b_name = bmap[book[0]]\n",
    "        rel = c1_name + ' AND ' + c2_name\n",
    "        \n",
    "        if rel not in interest_pairs:\n",
    "            continue\n",
    "            \n",
    "        if rel != \"U.S. AND China\" and rel != \"U.S. AND Russia\" and rel != \"U.S. AND India\" and rel != \"U.S. AND Syria\":\n",
    "            continue\n",
    "            \n",
    "        char1 = [chars[0]]\n",
    "        char2 = [chars[1]]\n",
    "        model.zero_grad()\n",
    "        model.init_attention(A_dict[(chars[0], chars[1])].to(device))\n",
    "        \n",
    "        # starting attention calculation\n",
    "        attn_seaborn_d = dict()\n",
    "        attn_score_dict = defaultdict(dict) # structure: (desc_i, month_i): (attended word): [(score, span_index)]\n",
    "        attn_selected_sample_dict[b_name][rel] = defaultdict(dict)\n",
    "        \n",
    "        base_seaborn_d = dict()\n",
    "        base_count_dict = defaultdict(dict) # structure: (desc_i, month_i): (word): (word count)\n",
    "        overall_base_count_dict = dict()\n",
    "        \n",
    "        # calculate attention score for each document\n",
    "        for span_index, (span, mask, month) in enumerate(zip(spans, masks, months)):\n",
    "            prom_i = np.argmax(desc_dist_dict[rel][span_index]) # give attention record to the prominent descriptor\n",
    "            \n",
    "            candidate_set = set()\n",
    "            for i in range(len(span)):\n",
    "                if mask[i] == noun_ix:\n",
    "                    candidate_set.add((span[i], month))\n",
    "                    if revmap[span[i]] not in base_count_dict[(prom_i, month)].keys():\n",
    "                        base_count_dict[(prom_i, month)][revmap[span[i]]] = 1\n",
    "                    else:\n",
    "                        base_count_dict[(prom_i, month)][revmap[span[i]]] += 1\n",
    "                    if prom_i not in overall_base_count_dict:\n",
    "                        overall_base_count_dict[prom_i] = defaultdict(int)\n",
    "                    overall_base_count_dict[prom_i][revmap[span[i]]] += 1\n",
    "\n",
    "            if len(candidate_set) == 0:\n",
    "                continue\n",
    "\n",
    "            noun_list = []\n",
    "            month_list = []\n",
    "            for candidate in candidate_set:\n",
    "                noun_list.append(candidate[0])\n",
    "                month_list.append(candidate[1])\n",
    "\n",
    "            noun_part_mat = model.l_noun.we(torch.LongTensor(noun_list).to(device))\n",
    "            month_part_mat = torch.zeros([len(noun_part_mat), d_noun_hidden - d_word], dtype=torch.float).to(device)\n",
    "            for i, month in enumerate(month_list):\n",
    "                month_part_mat[i][month] = 1\n",
    "\n",
    "            hidden_mat = torch.cat((noun_part_mat, month_part_mat), 1)\n",
    "\n",
    "            key_mat = model.l_noun.linear_key(hidden_mat)\n",
    "            key_mat = torch.tanh(key_mat)\n",
    "\n",
    "            query_mat = model.l_noun.query(torch.LongTensor([i for i in range(1)]).to(device)) # only one q_vec\n",
    "\n",
    "            alpha_mat = torch.mm(query_mat, torch.t(key_mat))\n",
    "            alpha_mat = F.softmax(alpha_mat, dim=1)\n",
    "            alpha_mat = alpha_mat.detach().cpu().numpy()\n",
    "            \n",
    "            alpha_row = alpha_mat[0] # only one q_vec\n",
    "\n",
    "            for j, score in enumerate(alpha_row):\n",
    "                if revmap[noun_list[j]] in attn_score_dict[(prom_i, month)].keys():\n",
    "                    attn_score_dict[(prom_i, month)][revmap[noun_list[j]]].append((score, span_index))\n",
    "                else:\n",
    "                    attn_score_dict[(prom_i, month)][revmap[noun_list[j]]] = [(score, span_index)]\n",
    "                    \n",
    "        # rebuild the alpha mat for back compatibility\n",
    "        noun_list = []\n",
    "        month_list = []\n",
    "        noun_month_set = set()\n",
    "        for desc_i, month_i in attn_score_dict.keys():\n",
    "            for attn_word in attn_score_dict[(desc_i, month_i)].keys():\n",
    "                # START - A fix for previous bug of duplicate (noun, month) pair in alpha_mat\n",
    "                if (attn_word, month_i) in noun_month_set:\n",
    "                    continue\n",
    "                else:\n",
    "                    noun_month_set.add((attn_word, month_i))\n",
    "                # - END\n",
    "                noun_list.append(attn_word)\n",
    "                month_list.append(month_i)\n",
    "        alpha_mat = np.zeros([num_descs, len(noun_list)])\n",
    "        for i in range(num_descs):\n",
    "            for j, (noun, month_i) in enumerate(zip(noun_list, month_list)):\n",
    "                if noun not in attn_score_dict[(i, month_i)]:\n",
    "                    continue\n",
    "                _scores = [score for score, span_index in attn_score_dict[(i, month_i)][noun]]\n",
    "                alpha_mat[i][j] = np.mean(_scores) * np.log(len(_scores)) # attention score definition\n",
    "                \n",
    "        # original processing for common and special attended words\n",
    "        for i, row in enumerate(alpha_mat):\n",
    "            attn_seaborn_d[i] = {'month_info': [], 'attn_score': [], 'attn_word': []}\n",
    "\n",
    "            all_nouns = defaultdict(list)\n",
    "            # overall_score_dict = defaultdict(float)\n",
    "            overall_score_dict = defaultdict(list)\n",
    "\n",
    "            for j in np.argsort(row)[::-1]:\n",
    "                month_str = month_to_str(month_list[j], year_base)\n",
    "                all_nouns[month_str].append((noun_list[j], row[j]))\n",
    "                # overall_score_dict[noun_list[j]] += row[j]\n",
    "                overall_score_dict[noun_list[j]].append(row[j])\n",
    "\n",
    "            keyword_list = []\n",
    "\n",
    "            # add overall highest scored word to keyword list\n",
    "            # for w, score in sorted(list(overall_score_dict.items()), key=lambda x: -x[1])[:num_nouns_choice]:\n",
    "            for w, score in sorted(list(overall_score_dict.items()), key=lambda x: -np.sum(x[1]))[:num_nouns_choice]:\n",
    "                keyword_list.append(w)\n",
    "\n",
    "            # build seaborn dict\n",
    "            for m in sorted([str_m for str_m in all_nouns.keys()],\n",
    "                            key=lambda x: int(x[:2]) * 12 + int(x[-2:]) if len(x) == 5\n",
    "                            else int(x[:2]) * 12 + int(x[-1:])):\n",
    "                for w in keyword_list:\n",
    "                    target_score = 0\n",
    "                    for tw, tscore in all_nouns[m]:\n",
    "                        if tw == w:\n",
    "                            target_score = tscore\n",
    "                            break\n",
    "                    attn_seaborn_d[i]['month_info'].append(m)\n",
    "                    attn_seaborn_d[i]['attn_score'].append(target_score)\n",
    "                    attn_seaborn_d[i]['attn_word'].append(w)\n",
    "                    if w not in attn_score_dict[(i, str_to_month(m, year_base))].keys():\n",
    "                        continue\n",
    "                    w_sample_indices = [span_index for score, span_index in sorted(\n",
    "                        attn_score_dict[(i, str_to_month(m, year_base))][w], key=lambda x: -x[0])]\n",
    "                    if len(w_sample_indices) < sample_sel_num:\n",
    "                        continue\n",
    "                    attn_selected_sample_dict[b_name][rel][(i, str_to_month(m, year_base))][w]\\\n",
    "                        = [sample_dict[b_name][rel][ind] for ind in w_sample_indices[:sample_sel_num]]\n",
    "\n",
    "            attn_seaborn_d[i]['keyword_list'] = keyword_list\n",
    "            attn_seaborn_d[i]['month_list'] = [m for m in sorted([str_m for str_m in all_nouns.keys()],\n",
    "                            key=lambda x: int(x[:2]) * 12 + int(x[-2:]) if len(x) == 5\n",
    "                            else int(x[:2]) * 12 + int(x[-1:]))]\n",
    "            \n",
    "            # generate baseline's dictionary for seaborn figure\n",
    "            base_seaborn_d[i] = {'month_info': [], 'base_score': [], 'base_word': []}\n",
    "            keyword_list = []\n",
    "            for w, score in sorted(list(overall_base_count_dict[i].items()), key=lambda x: -x[1])[:num_nouns_choice]:\n",
    "                keyword_list.append(w)\n",
    "            for m in attn_seaborn_d[i]['month_list']:\n",
    "                for w in keyword_list:\n",
    "                    target_score = 0\n",
    "                    if w in base_count_dict[(i, str_to_month(m, year_base))].keys():\n",
    "                        target_score = base_count_dict[(i, str_to_month(m, year_base))][w]\n",
    "                    base_seaborn_d[i]['month_info'].append(m)\n",
    "                    base_seaborn_d[i]['base_score'].append(target_score)\n",
    "                    base_seaborn_d[i]['base_word'].append(w)\n",
    "\n",
    "            base_seaborn_d[i]['keyword_list'] = keyword_list\n",
    "            base_seaborn_d[i]['month_list'] = attn_seaborn_d[i]['month_list']\n",
    "\n",
    "        vis_dict[rel].append(attn_seaborn_d)\n",
    "        vis_dict[rel].append(base_seaborn_d)\n",
    "        \n",
    "pickle.dump(attn_selected_sample_dict, open(attn_sample_file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_choices = ['blue', 'red', 'green', 'orange', 'grey', 'purple']\n",
    "desc_palette = dict()\n",
    "\n",
    "desc_pie_sizes = []\n",
    "desc_pie_labels = []\n",
    "\n",
    "sum_all_shares = sum([v for k, v in major_desc_sum_dict.items()])\n",
    "for i, (desc_i, sum_share) in enumerate(sorted([(k, v) for k, v in major_desc_sum_dict.items()], key=lambda x: -x[1])):\n",
    "    print(rmn_descs[desc_i], '\\t\\t', sum_share / sum_all_shares)\n",
    "    desc_palette[desc_i] = color_choices[i % len(color_choices)]\n",
    "    desc_pie_sizes.append(sum_share)\n",
    "    desc_pie_labels.append(rmn_descs[desc_i])\n",
    "sdps = sum(desc_pie_sizes)\n",
    "for i in range(len(desc_pie_sizes)):\n",
    "    desc_pie_sizes[i] /= sdps\n",
    "    \n",
    "sns.set_context(\"notebook\", font_scale=2.2)\n",
    "plt.figure(figsize=(10,18))\n",
    "patches, _, _ = plt.pie(desc_pie_sizes, autopct='%1.0f%%', startangle=90, colors=color_choices, textprops=dict(color=\"w\",fontsize='small'))\n",
    "plt.legend(patches, desc_pie_labels, loc=\"best\")\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2er_visualization(b_name, rel, desc_share_d, attn_dict, baseline_d, rmn_descs, desc_palette, key_event_dict):\n",
    "    print('##################', rel, '##################')\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_context(\"notebook\", font_scale=3.2, rc={\"lines.linewidth\": 2}) # previous font: 2.2\n",
    "    if len(desc_share_d['desc_type']) == 0:\n",
    "        return\n",
    "    desc_share_df = pd.DataFrame(data=desc_share_d)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    ax = sns.pointplot(data=desc_share_df, x='month_info', y='desc_share', hue='desc_type',\n",
    "                       ci=None, palette=desc_palette, markers=['o', 'x', '^'], scale=1.5)\n",
    "    plt.setp([ax.get_lines()],alpha=.5)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(0, 1.3))\n",
    "    new_title = 'Descriptor Type'\n",
    "    ax.legend_.set_title(new_title)\n",
    "    for t in ax.legend_.texts:\n",
    "        t.set_text(rmn_descs[int(t.get_text())].split(',')[0])\n",
    "    key_event_count = 0\n",
    "    for xl in ax.get_xticklabels():\n",
    "        if xl.get_text()[2:] in key_event_dict[b_name][rel].keys():\n",
    "            key_event_text = key_event_dict[b_name][rel][xl.get_text()[2:]].split('(')[0]\n",
    "            x_coor = str_to_month(xl.get_text()[2:], year_base) - 1\n",
    "            plt.axvline(x=x_coor, linestyle='--', color='black', alpha=0.5, lw = 2)\n",
    "#             trans = ax.get_xaxis_transform() # x in data untis, y in axes fraction\n",
    "#             ax.annotate(key_event_text, xy=(x_coor, -0.18),  xycoords=trans,\n",
    "#                         xytext=(0, -.325 - (key_event_count) * .15), textcoords=trans,\n",
    "#                         bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n",
    "#                         arrowprops=dict(arrowstyle=\"-|>\", lw=2,\n",
    "#                                         connectionstyle=\"angle,angleA=0,angleB=80,rad=20\"))\n",
    "            key_event_count += 1\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "    ax.xaxis.set_label_text('')\n",
    "    ax.yaxis.set_label_text('Descriptor Weight')\n",
    "    plt.show()\n",
    "    #ax.figure.savefig('figures/' + rel + '.png', dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    curr_non_trivial_descs = set(desc_share_d['desc_type'])\n",
    "    for k in attn_dict.keys():\n",
    "        if k not in curr_non_trivial_descs:\n",
    "            continue\n",
    "        title = rmn_descs[k]\n",
    "        print(title)\n",
    "        attn_df = pd.DataFrame(data={'attn_word': attn_dict[k]['attn_word'],\n",
    "                                     'month_info': attn_dict[k]['month_info'],\n",
    "                                     'attn_score': attn_dict[k]['attn_score']})\n",
    "        keyword_list = attn_dict[k]['keyword_list']\n",
    "        month_list = attn_dict[k]['month_list']\n",
    "        attn_array = np.zeros([len(keyword_list), len(month_list)])\n",
    "        max_attn_score = 0\n",
    "        for i, keyword in enumerate(keyword_list):\n",
    "            for j, month in enumerate(month_list):\n",
    "                attn_array[i][j] = attn_df[(attn_df.attn_word == keyword) &\n",
    "                                           (attn_df.month_info == month)]['attn_score']\n",
    "                if attn_array[i][j] > max_attn_score:\n",
    "                    max_attn_score = attn_array[i][j]\n",
    "        max_column_attn_scores = [np.max([attn_array[_i][j] for _i in range(len(attn_array))])\n",
    "                                  for j, month in enumerate(month_list)]\n",
    "        for i, keyword in enumerate(keyword_list): # normalization\n",
    "            max_row_attn_score = np.max(attn_array[i])\n",
    "            for j, month in enumerate(month_list):\n",
    "                attn_array[i][j] = 1.0 * attn_array[i][j] / max_attn_score\n",
    "                # alt: use max_attn_score for global normalization\n",
    "        plt.figure(figsize=(18,len(keyword_list)/1.8))\n",
    "#         print(attn_array.mean(axis=1))\n",
    "        ax = sns.heatmap(attn_array, xticklabels=['20'+_m for _m in month_list], yticklabels=keyword_list,\n",
    "                         linewidths=.05, linecolor='dimgray', cmap=desc_palette[k].capitalize()+'s', cbar=False)\n",
    "        key_event_count = 0\n",
    "        for xl in ax.get_xticklabels():\n",
    "            if xl.get_text()[2:] in key_event_dict[b_name][rel].keys():\n",
    "                key_event_text = key_event_dict[b_name][rel][xl.get_text()[2:]].split('(')[0]\n",
    "#                 x_coor = str_to_month(xl.get_text()[2:], year_base) - 1\n",
    "#                 trans = ax.get_xaxis_transform() # x in data untis, y in axes fraction\n",
    "#                 ax.annotate(key_event_text, xy=(x_coor + .5, -.5),  xycoords=trans,\n",
    "#                             xytext=(0, -.75 - (key_event_count) * .25), textcoords=trans,\n",
    "#                             bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n",
    "#                             arrowprops=dict(arrowstyle=\"-|>\", lw=2,\n",
    "#                                             connectionstyle=\"angle,angleA=0,angleB=80,rad=20\"))\n",
    "                key_event_count += 1\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "        plt.show()\n",
    "#         ax.figure.savefig('figures/_' + rel + '_attn_' + str(k) + '.png', dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # word frequency baseline\n",
    "        title = rmn_descs[k] + ' (Baseline)'\n",
    "        print(title)\n",
    "        attn_df = pd.DataFrame(data={'base_word': baseline_d[k]['base_word'],\n",
    "                                     'month_info': baseline_d[k]['month_info'],\n",
    "                                     'base_score': baseline_d[k]['base_score']})\n",
    "        keyword_list = baseline_d[k]['keyword_list']\n",
    "        month_list = baseline_d[k]['month_list']\n",
    "        attn_array = np.zeros([len(keyword_list), len(month_list)])\n",
    "        max_attn_score = 0\n",
    "        for i, keyword in enumerate(keyword_list):\n",
    "            for j, month in enumerate(month_list):\n",
    "                attn_array[i][j] = attn_df[(attn_df.base_word == keyword) &\n",
    "                                           (attn_df.month_info == month)]['base_score']\n",
    "                if attn_array[i][j] > max_attn_score:\n",
    "                    max_attn_score = attn_array[i][j]\n",
    "        max_column_attn_scores = [np.max([attn_array[_i][j] for _i in range(len(attn_array))])\n",
    "                                  for j, month in enumerate(month_list)]\n",
    "        for i, keyword in enumerate(keyword_list): # normalization\n",
    "            max_row_attn_score = np.max(attn_array[i])\n",
    "            for j, month in enumerate(month_list):\n",
    "                attn_array[i][j] = 1.0 * attn_array[i][j] / max_attn_score\n",
    "                # alt: use max_attn_score for global normalization\n",
    "        plt.figure(figsize=(18,len(keyword_list)/1.8))\n",
    "\n",
    "        ax = sns.heatmap(attn_array, xticklabels=['20'+_m for _m in month_list], yticklabels=keyword_list,\n",
    "                         linewidths=.05, linecolor='dimgray', cmap=desc_palette[k].capitalize()+'s', cbar=False)\n",
    "        key_event_count = 0\n",
    "        for xl in ax.get_xticklabels():\n",
    "            if xl.get_text()[2:] in key_event_dict[b_name][rel].keys():\n",
    "                key_event_text = key_event_dict[b_name][rel][xl.get_text()[2:]].split('(')[0]\n",
    "#                 x_coor = str_to_month(xl.get_text()[2:], year_base) - 1\n",
    "#                 trans = ax.get_xaxis_transform() # x in data untis, y in axes fraction\n",
    "#                 ax.annotate(key_event_text, xy=(x_coor + .5, -.5),  xycoords=trans,\n",
    "#                             xytext=(0, -.75 - (key_event_count) * .25), textcoords=trans,\n",
    "#                             bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n",
    "#                             arrowprops=dict(arrowstyle=\"-|>\", lw=2,\n",
    "#                                             connectionstyle=\"angle,angleA=0,angleB=80,rad=20\"))\n",
    "                key_event_count += 1\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "        plt.show()\n",
    "#         ax.figure.savefig('figures/_' + rel + '_freq_' + str(k) + '.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for rel in vis_dict.keys():\n",
    "#     e2er_visualization('Internation', rel, vis_dict[rel][0], dict(), dict(),\n",
    "#                        rmn_descs, desc_palette, key_event_dict)\n",
    "    e2er_visualization('Internation', rel, vis_dict[rel][0], vis_dict[rel][1], vis_dict[rel][2],\n",
    "                       rmn_descs, desc_palette, key_event_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For change point analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to a new notebook to do this\n",
    "pickle.dump((vis_dict, key_event_dict), open('change_point_resource_we.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples With High Descriptor Score\n",
    "descriptor 20: denounce\n",
    "\n",
    "descriptor 9: strengthen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# param: entity pair, descriptor index, month\n",
    "desc_query(desc_sample_file_name, \"U.S. AND China\", 20, '18-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_query(desc_sample_file_name, \"U.S. AND China\", 9, '18-03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples With High Attention Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: entity pair, descriptor index, month, word\n",
    "attn_query(attn_sample_file_name, \"U.S. AND China\", 20, '17-02', \"tariff\") # denounce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention score on single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_rel = \"U.S. AND China\"\n",
    "tar_month = 27\n",
    "tar_sample = \"China says ready to defend its interests in US trade spat\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for book, chars, spans, masks, months, samples in span_data:\n",
    "        c1_name, c2_name = [cmap[c] for c in chars]\n",
    "        b_name = bmap[book[0]]\n",
    "        rel = c1_name + ' AND ' + c2_name\n",
    "        \n",
    "        if rel != tar_rel:\n",
    "            continue\n",
    "            \n",
    "        char1 = [chars[0]]\n",
    "        char2 = [chars[1]]\n",
    "        model.zero_grad()\n",
    "        model.init_attention(A_dict[(chars[0], chars[1])].to(device))\n",
    "        \n",
    "        # calculate attention score for each document\n",
    "        for span_index, (span, mask, month, sample) in enumerate(zip(spans, masks, months, samples)):\n",
    "            if month != tar_month:\n",
    "                continue\n",
    "                \n",
    "            if tar_sample not in sample[0]:\n",
    "                continue\n",
    "                \n",
    "            print(sample[0], '\\n')\n",
    "            \n",
    "            candidate_set = set()\n",
    "            for i in range(len(span)):\n",
    "                if mask[i] == predicate_ix:\n",
    "                    print(revmap[span[i]])\n",
    "                if mask[i] == noun_ix:\n",
    "                    candidate_set.add((span[i], month))\n",
    "\n",
    "            if len(candidate_set) == 0:\n",
    "                continue\n",
    "\n",
    "            noun_list = []\n",
    "            month_list = []\n",
    "            for candidate in candidate_set:\n",
    "                noun_list.append(candidate[0])\n",
    "                month_list.append(candidate[1])\n",
    "\n",
    "            noun_part_mat = model.l_noun.we(torch.LongTensor(noun_list).to(device))\n",
    "            month_part_mat = torch.zeros([len(noun_part_mat), d_noun_hidden - d_word], dtype=torch.float).to(device)\n",
    "            for i, month in enumerate(month_list):\n",
    "                month_part_mat[i][month] = 1\n",
    "\n",
    "            hidden_mat = torch.cat((noun_part_mat, month_part_mat), 1)\n",
    "\n",
    "            key_mat = model.l_noun.linear_key(hidden_mat)\n",
    "            key_mat = torch.tanh(key_mat)\n",
    "\n",
    "            query_mat = model.l_noun.query(torch.LongTensor([i for i in range(1)]).to(device)) # only one q_vec\n",
    "\n",
    "            alpha_mat = torch.mm(query_mat, torch.t(key_mat))\n",
    "            alpha_mat = F.softmax(alpha_mat, dim=1)\n",
    "            alpha_mat = alpha_mat.detach().cpu().numpy()\n",
    "            \n",
    "            alpha_row = alpha_mat[0] # only one q_vec\n",
    "\n",
    "            print('\\n', sorted([(revmap[noun_list[j]], score) for j, score in enumerate(alpha_row)], key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
