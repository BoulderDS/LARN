{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "from utils import *\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'rmn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmap, cmap, wmap, revmap, span_data, span_size, target_pred_ix_set, We = load_data(data_load_path)\n",
    "\n",
    "num_chars, num_books, num_traj = len(cmap), len(bmap), len(span_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_log = 'descriptors_model_' + str(model_id) + '.log'\n",
    "\n",
    "trajectory_log = 'trajectories_model_' + str(model_id) + '.log'\n",
    "\n",
    "training_progress_log = 'training_progress_log_' + str(model_id) + '.txt'\n",
    "\n",
    "model_object_file_name = 'trained_model_' + str(model_id) + '.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMN(nn.Module):\n",
    "\n",
    "    def __init__(self, d_word, d_ent, d_meta, d_mix, d_desc, n_ent, n_meta, We):\n",
    "        super(RMN, self).__init__()\n",
    "        self.d_desc = d_desc\n",
    "        self.l_st = TrainedWordEmbeddingLayer(torch.FloatTensor(We).to(device), d_word)\n",
    "        self.l_ent = nn.Embedding(n_ent, d_ent)\n",
    "        self.l_meta = nn.Embedding(n_meta, d_meta)\n",
    "        self.l_mix = MixingLayer(d_word, d_ent, d_meta, d_mix)\n",
    "        self.l_rels = LinearRNN(d_mix, d_desc)\n",
    "        self.l_recon = nn.Linear(d_desc, d_word)\n",
    "        nn.init.xavier_uniform_(self.l_recon.weight)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        self.l_rels.begin = True\n",
    "\n",
    "    def forward(self, spans, e1, e2, meta):\n",
    "        e1_tensor = torch.LongTensor(e1).to(device)\n",
    "        e2_tensor = torch.LongTensor(e2).to(device)\n",
    "        meta_tensor = torch.LongTensor(meta).to(device)\n",
    "        \n",
    "        outputs_l_st = self.l_st(spans)\n",
    "        outputs_l_e1 = self.l_ent(e1_tensor).expand(len(spans), -1)\n",
    "        outputs_l_e2 = self.l_ent(e2_tensor).expand(len(spans), -1)\n",
    "        outputs_l_meta = self.l_meta(meta_tensor).expand(len(spans), -1)\n",
    "        \n",
    "        outputs_l_mix = self.l_mix(outputs_l_st, outputs_l_e1, outputs_l_e2, outputs_l_meta)\n",
    "        \n",
    "        outputs_l_rels = torch.zeros([len(spans), self.d_desc], dtype=torch.float).to(device)\n",
    "        for i, olm in enumerate(outputs_l_mix):\n",
    "            outputs_l_rels[i], self.l_rels.hidden = self.l_rels(olm, self.l_rels.hidden)\n",
    "        \n",
    "        outputs = self.l_recon(outputs_l_rels)\n",
    "        return outputs, outputs_l_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RMN(d_word, d_char, d_book, d_mix, num_descs, num_chars, num_books, We).to(device)\n",
    "loss_function = Contrastive_Max_Margin_Loss().to(device)\n",
    "optimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()), lr=lr)\n",
    "label_generation_layer = TrainedWordEmbeddingLayer(torch.FloatTensor(We), d_word).to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_start_t = time.time()\n",
    "    epoch_total_loss = torch.tensor(0.).to(device)\n",
    "    random.shuffle(span_data)\n",
    "    \n",
    "    for book, chars, big_spans, big_masks, big_months, _ in span_data:\n",
    "        \n",
    "        split_indices = [i for i in range(0, len(big_spans), batch_size)] # split to batches to fit in memory\n",
    "        spans_split = np.split(big_spans, split_indices)\n",
    "        masks_split = np.split(big_masks, split_indices)\n",
    "        months_split = np.split(big_months, split_indices)\n",
    "        for spans, masks, months in zip(spans_split, masks_split, months_split):\n",
    "            if len(spans) != batch_size:\n",
    "                continue\n",
    "        \n",
    "            char1 = [chars[0]]\n",
    "            char2 = [chars[1]]\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.init_hidden()\n",
    "\n",
    "            train_masked_spans = []\n",
    "            drop_masks = (np.random.rand(*(masks.shape)) < (1 - p_drop)).astype('float32')\n",
    "            for span_index, (span, mask, drop_mask) in enumerate(zip(spans, masks, drop_masks)):\n",
    "                train_masked_span = [span[i] for i in range(len(span))\n",
    "                                     if (mask[i] == all_ix)\n",
    "                                     and (drop_mask[i] == 1)]\n",
    "                train_masked_spans.append(train_masked_span)\n",
    "\n",
    "            pos_masked_spans = []\n",
    "            for span_index, (span, mask) in enumerate(zip(spans, masks)):\n",
    "                pos_masked_span = [span[i] for i in range(len(span))\n",
    "                                     if (mask[i] == all_ix)]\n",
    "                pos_masked_spans.append(pos_masked_span)\n",
    "\n",
    "            neg_spans, neg_masks = generate_negative_samples(num_traj, span_size,\n",
    "                                                             num_negs, span_data)\n",
    "            neg_masked_spans = []\n",
    "            for span_index, (span, mask) in enumerate(zip(neg_spans, neg_masks)):\n",
    "                neg_masked_span = [span[i] for i in range(len(span))\n",
    "                                     if (mask[i] == all_ix)]\n",
    "                neg_masked_spans.append(neg_masked_span)\n",
    "\n",
    "            outputs, _outputs_l_rels = model(train_masked_spans, char1, char2, book)\n",
    "            pos_labels = label_generation_layer(pos_masked_spans)\n",
    "            neg_labels = label_generation_layer(neg_masked_spans)\n",
    "\n",
    "            R = torch.t(model.l_recon.weight)\n",
    "\n",
    "            loss = loss_function(outputs, pos_labels, neg_labels, len(spans), R, eps, d_word)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_total_loss += loss\n",
    "    \n",
    "    print(\"epoch %d finished in %.2f seconds\" % (epoch, (time.time() - epoch_start_t)))\n",
    "    print(epoch_total_loss)\n",
    "    f_tpl = open(training_progress_log, 'a')\n",
    "    f_tpl.write('epoch ' + str(epoch) + ': ' + str(time.time() - epoch_start_t) + '\\n')\n",
    "    f_tpl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_object_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_object_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptors Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_ix_set = set()\n",
    "target_word_ix_counter = Counter()\n",
    "\n",
    "for _, _, spans, masks, _ , _ in span_data:\n",
    "    for span, mask in zip(spans, masks):\n",
    "        for w, m in zip(span, mask):\n",
    "            if m == all_ix:\n",
    "                target_word_ix_counter[w] += 1\n",
    "                \n",
    "for wix, _ in target_word_ix_counter.most_common()[:500]: # can also do without the 500 top word limit, but less interpretable\n",
    "    target_word_ix_set.add(wix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log = open(descriptor_log, 'w')\n",
    "\n",
    "R = torch.t(model.l_recon.weight).detach().cpu().numpy()\n",
    "for ind in range(len(R)):\n",
    "    desc = R[ind] / np.linalg.norm(R[ind])\n",
    "    top_desc_list = []\n",
    "    \n",
    "    sims = We.dot(np.transpose(desc))\n",
    "    ordered_words = np.argsort(sims)[::-1]\n",
    "\n",
    "    desc_list = []\n",
    "    num_satisfied = 0\n",
    "    for w in ordered_words:\n",
    "        if num_satisfied >= num_descs_choice:\n",
    "            break\n",
    "        if w in target_word_ix_set:\n",
    "            desc_list.append(revmap[w])\n",
    "            num_satisfied += 1\n",
    "    \n",
    "    sentiment_score = calc_desc_sentiment(desc_list)\n",
    "    top_desc_list.append('-'.join(desc_list) + ' [' + str(sentiment_score) + ']')\n",
    "\n",
    "    print('descriptor %d:' % ind)\n",
    "    print(top_desc_list)\n",
    "    print()\n",
    "\n",
    "    log.write(' '.join(top_desc_list) + '\\n')\n",
    "log.flush()\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Trend Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict = defaultdict(dict)\n",
    "\n",
    "tlog = open(trajectory_log, 'w')\n",
    "traj_writer = csv.writer(tlog)\n",
    "traj_writer.writerow(['Book', 'Char 1', 'Char 2', 'Span ID'] + ['Topic ' + str(i) for i in range(num_descs)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for book, chars, spans, masks, months, samples in span_data:\n",
    "        c1_name, c2_name = [cmap[c] for c in chars]\n",
    "        b_name = bmap[book[0]]\n",
    "\n",
    "        sample_dict[b_name][c1_name+' AND '+c2_name] = dict()\n",
    "        for span_index, sample in enumerate(samples):\n",
    "            sample_dict[b_name][c1_name+' AND '+c2_name][span_index] = sample\n",
    "\n",
    "        char1 = [chars[0]]\n",
    "        char2 = [chars[1]]\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        masked_spans = []\n",
    "        for span_index, (span, mask) in enumerate(zip(spans, masks)):\n",
    "            masked_span = [span[i] for i in range(len(span))\n",
    "                                     if (mask[i] == all_ix)]\n",
    "            masked_spans.append(masked_span)\n",
    "\n",
    "        _outputs, outputs_l_rels = model(masked_spans, char1, char2, book)\n",
    "\n",
    "        for span_index, olr in enumerate(outputs_l_rels.detach().cpu().numpy()):\n",
    "            traj_writer.writerow([b_name, c1_name, c2_name, months[span_index], span_index] + [o for o in olr])\n",
    "\n",
    "tlog.flush()\n",
    "tlog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dict = dict()\n",
    "num_top_descs = 3 # to get an overview for all descriptors, change this to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_selected_sample_dict = defaultdict(dict) # key: (int_desc_index, int_month_info)\n",
    "major_desc_sum_dict = defaultdict(float)\n",
    "\n",
    "rmn_traj = read_csv(trajectory_log)\n",
    "rmn_descs = read_descriptors(descriptor_log)\n",
    "\n",
    "for book in rmn_traj:\n",
    "    for rel in rmn_traj[book]:\n",
    "        if rel not in interest_pairs:\n",
    "            continue\n",
    "            \n",
    "        desc_selected_sample_dict[book][rel] = defaultdict(list)\n",
    "        plt.close()\n",
    "        \n",
    "        rtraj = rmn_traj[book][rel]['distributions']\n",
    "        mtraj = rmn_traj[book][rel]['months']\n",
    "        itraj = rmn_traj[book][rel]['span_index']\n",
    "        \n",
    "        data_d = dict()\n",
    "        desc_sum_d = defaultdict(float)\n",
    "        trivial_descs = set()\n",
    "        \n",
    "        # find non-trivial descs\n",
    "        for i in range(num_descs):\n",
    "            data_d[i] = defaultdict(list)\n",
    "        for r, m in zip(rtraj, mtraj):\n",
    "            for i, desc in enumerate(r):\n",
    "                data_d[i][m].append(desc)\n",
    "        for i in data_d.keys():\n",
    "            trivial = True\n",
    "            for m in data_d[i].keys():\n",
    "                desc_sum_d[i] += np.mean(data_d[i][m])\n",
    "\n",
    "        top_ds = [top_d[0] for top_d in sorted([(i, share) for i, share in desc_sum_d.items()],\n",
    "                                               key=lambda x: -x[1])[:num_top_descs]] # get top three descriptors\n",
    "        \n",
    "        seaborn_d = {'month_info': [], 'desc_share': [], 'desc_type': []}\n",
    "        desc_share_dict = defaultdict(list)\n",
    "        \n",
    "        for r, m, span_index in zip(rtraj, mtraj, itraj):\n",
    "            for i, desc in enumerate(r):\n",
    "                if i not in top_ds:\n",
    "                    continue\n",
    "                desc_share_dict[(i, m)].append((desc, span_index))\n",
    "                seaborn_d['month_info'].append('20' + month_to_str(m, year_base))\n",
    "                seaborn_d['desc_share'].append(desc)\n",
    "                seaborn_d['desc_type'].append(i)\n",
    "                major_desc_sum_dict[i] += desc\n",
    "        \n",
    "        vis_dict[rel] = [seaborn_d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_palette = dict()\n",
    "color_choices = ['blue', 'green', 'grey', 'purple', 'red', 'yellow']\n",
    "\n",
    "desc_pie_sizes = []\n",
    "desc_pie_labels = []\n",
    "\n",
    "sum_all_shares = sum([v for k, v in major_desc_sum_dict.items()])\n",
    "for i, (desc_i, sum_share) in enumerate(sorted([(k, v) for k, v in major_desc_sum_dict.items()], key=lambda x: -x[1])):\n",
    "    print(rmn_descs[desc_i], '\\t\\t', sum_share / sum_all_shares)\n",
    "    desc_palette[desc_i] = color_choices[i % len(color_choices)]\n",
    "    desc_pie_sizes.append(sum_share)\n",
    "    desc_pie_labels.append(rmn_descs[desc_i])\n",
    "sdps = sum(desc_pie_sizes)\n",
    "for i in range(len(desc_pie_sizes)):\n",
    "    desc_pie_sizes[i] /= sdps\n",
    "    \n",
    "sns.set_context(\"notebook\", font_scale=2.2)\n",
    "plt.figure(figsize=(10,18))\n",
    "patches, _, _ = plt.pie(desc_pie_sizes, autopct='%1.0f%%', startangle=90, colors=color_choices, textprops=dict(color=\"w\",fontsize='small'))\n",
    "plt.legend(patches, desc_pie_labels, loc=\"best\")\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2er_visualization(b_name, rel, desc_share_d, attn_dict, baseline_d, rmn_descs, desc_palette, key_event_dict):\n",
    "    print('##################', rel, '##################')\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_context(\"notebook\", font_scale=3.2, rc={\"lines.linewidth\": 2}) # previous font: 2.2\n",
    "    if len(desc_share_d['desc_type']) == 0:\n",
    "        return\n",
    "    desc_share_df = pd.DataFrame(data=desc_share_d)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    ax = sns.pointplot(data=desc_share_df, x='month_info', y='desc_share', hue='desc_type',\n",
    "                       ci=None, palette=desc_palette, markers=['o', 'x', '^'], scale=1.5)\n",
    "    plt.setp([ax.get_lines()],alpha=.5)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(0, 1.3))\n",
    "    new_title = 'Descriptor Type'\n",
    "    ax.legend_.set_title(new_title)\n",
    "    for t in ax.legend_.texts:\n",
    "        t.set_text(rmn_descs[int(t.get_text())].split(',')[0])\n",
    "    key_event_count = 0\n",
    "    for xl in ax.get_xticklabels():\n",
    "        if xl.get_text()[2:] in key_event_dict[b_name][rel].keys():\n",
    "            key_event_text = key_event_dict[b_name][rel][xl.get_text()[2:]].split('(')[0]\n",
    "            x_coor = str_to_month(xl.get_text()[2:], year_base) - 1\n",
    "            plt.axvline(x=x_coor, linestyle='--', color='black', alpha=0.5, lw = 2)\n",
    "#             trans = ax.get_xaxis_transform() # x in data untis, y in axes fraction\n",
    "#             ax.annotate(key_event_text, xy=(x_coor, -0.18),  xycoords=trans,\n",
    "#                         xytext=(0, -.325 - (key_event_count) * .15), textcoords=trans,\n",
    "#                         bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\n",
    "#                         arrowprops=dict(arrowstyle=\"-|>\", lw=2,\n",
    "#                                         connectionstyle=\"angle,angleA=0,angleB=80,rad=20\"))\n",
    "            key_event_count += 1\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "    ax.xaxis.set_label_text('')\n",
    "    ax.yaxis.set_label_text('Descriptor Weight')\n",
    "    plt.show()\n",
    "#     ax.figure.savefig('figures/mohit_' + rel + '.png', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for rel in vis_dict.keys():\n",
    "    e2er_visualization('Internation', rel, vis_dict[rel][0], dict(), dict(),\n",
    "                       rmn_descs, desc_palette, key_event_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For change point analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to a new notebook to do this\n",
    "pickle.dump((vis_dict, key_event_dict), open('change_point_resource_mohit.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
